{
    "version": "https://jsonfeed.org/version/1",
    "title": "minz的筆記本 • All posts by \"cerebras\" tag",
    "description": "這是一個分享我學習筆記的空間",
    "home_page_url": "https://blog.minz.li",
    "items": [
        {
            "id": "https://blog.minz.li/AI/Accelerators/",
            "url": "https://blog.minz.li/AI/Accelerators/",
            "title": "超越 GPU 在 Groq/Cerebras/SambaNova 使用高速輸出的 Deepseek",
            "date_published": "2025-02-19T16:00:00.000Z",
            "content_html": "<h1 id=\"groq-介紹\"><a class=\"anchor\" href=\"#groq-介紹\">#</a> Groq 介紹</h1>\n<p>Groq 的主要產品是 LPU (Language Processing Units)，能夠大幅提升模型的推論速度，這表示模型回答的速度加快，比 GPT-4o 快了數倍。<br />\nGroq 的 實驗性模型 <code>llama-3.3-70b-specdec</code> 在 GroqCloud 上的推論速度最快可以達到 <a href=\"https://groq.com/groq-first-generation-14nm-chip-just-got-a-6x-speed-boost-introducing-llama-3-1-70b-speculative-decoding-on-groqcloud/\">1,660 tokens/s</a> 。</p>\n<ul>\n<li>低延遲 (Seconds to First Token Received)</li>\n</ul>\n<p><img loading=\"lazy\" src=\"https://groq.com/wp-content/uploads/2024/11/latency-1536x762.jpg.webp\" alt=\"\" /></p>\n<ul>\n<li>高性能 (High Output Tokens per Second)</li>\n</ul>\n<p><img loading=\"lazy\" src=\"https://groq.com/wp-content/uploads/2024/11/output-speed2-1536x762.jpg.webp\" alt=\"\" /></p>\n<h2 id=\"可以使用的-models\"><a class=\"anchor\" href=\"#可以使用的-models\">#</a> 可以使用的 Models</h2>\n<ul>\n<li>可以在 <a href=\"https://console.groq.com/docs/models\">https://console.groq.com/docs/models</a> 查看可以使用的模型</li>\n<li>目前多為 llama、gemma、whisper 系列</li>\n</ul>\n<h2 id=\"使用\"><a class=\"anchor\" href=\"#使用\">#</a> 使用</h2>\n<p>可以在官方網站 <a href=\"https://groq.com/\">https://groq.com/</a> 或是<a href=\"https://console.groq.com/playground\">playground</a>直接使用</p>\n<p><img loading=\"lazy\" src=\"/images/AI/Accelerators/groq_home.webp\" alt=\"\" width=\"80%\" /></p>\n<h2 id=\"api\"><a class=\"anchor\" href=\"#api\">#</a> API</h2>\n<p><a href=\"https://console.groq.com/keys\">https://console.groq.com/keys</a><br />\n在左邊 API Keys 的選單中點擊 Create API Key，目前免費層級有提供一定的額度使用，API 兼容 openai 格式，修改 base_url 和 model 後可以直接使用。</p>\n<pre class=\"shiki shiki-themes vitesse-light vitesse-dark\" style=\"background-color:#ffffff;--shiki-dark-bg:#121212;color:#393a34;--shiki-dark:#dbd7caee\" tabindex=\"0\"><code class=\"language-python\"><span class=\"line\"><span style=\"color:#1E754F;--shiki-dark:#4D9375\">import</span><span style=\"color:#393A34;--shiki-dark:#DBD7CAEE\"> os</span></span>\n<span class=\"line\"><span style=\"color:#1E754F;--shiki-dark:#4D9375\">import</span><span style=\"color:#393A34;--shiki-dark:#DBD7CAEE\"> openai</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#393A34;--shiki-dark:#DBD7CAEE\">client </span><span style=\"color:#999999;--shiki-dark:#666666\">=</span><span style=\"color:#393A34;--shiki-dark:#DBD7CAEE\"> openai</span><span style=\"color:#999999;--shiki-dark:#666666\">.</span><span style=\"color:#393A34;--shiki-dark:#DBD7CAEE\">OpenAI</span><span style=\"color:#2993a3;--shiki-dark:#5eaab5\">(</span></span>\n<span class=\"line\"><span style=\"color:#B07D48;--shiki-dark:#BD976A\">    base_url</span><span style=\"color:#999999;--shiki-dark:#666666\">=</span><span style=\"color:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"color:#B56959;--shiki-dark:#C98A7D\">https://api.groq.com/openai/v1</span><span style=\"color:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"color:#999999;--shiki-dark:#666666\">,</span></span>\n<span class=\"line\"><span style=\"color:#B07D48;--shiki-dark:#BD976A\">    api_key</span><span style=\"color:#999999;--shiki-dark:#666666\">=</span><span style=\"color:#393A34;--shiki-dark:#DBD7CAEE\">os</span><span style=\"color:#999999;--shiki-dark:#666666\">.</span><span style=\"color:#393A34;--shiki-dark:#DBD7CAEE\">environ</span><span style=\"color:#999999;--shiki-dark:#666666\">.</span><span style=\"color:#393A34;--shiki-dark:#DBD7CAEE\">get</span><span style=\"color:#1e754f;--shiki-dark:#4d9375\">(</span><span style=\"color:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"color:#B56959;--shiki-dark:#C98A7D\">GROQ_API_KEY</span><span style=\"color:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"color:#1e754f;--shiki-dark:#4d9375\">)</span></span>\n<span class=\"line\"><span style=\"color:#2993a3;--shiki-dark:#5eaab5\">)</span></span></code></pre>\n<h1 id=\"cerebras-介紹\"><a class=\"anchor\" href=\"#cerebras-介紹\">#</a> Cerebras 介紹</h1>\n<p>Cerebras 研發的產品是 Wafer Scale Engine（WSE，晶圓級引擎），是一款 超大型 AI 加速晶片。</p>\n<ul>\n<li><a href=\"https://cerebras.ai/blog/cerebras-launches-worlds-fastest-deepseek-r1-llama-70b-inference\">推理速度</a></li>\n</ul>\n<p><img loading=\"lazy\" src=\"https://cerebras.ai/wp-content/uploads/2025/01/deepseek-chart.png\" alt=\"\" /></p>\n<h2 id=\"可以使用的-models-2\"><a class=\"anchor\" href=\"#可以使用的-models-2\">#</a> 可以使用的 Models</h2>\n<ul>\n<li>可以在 <a href=\"https://inference-docs.cerebras.ai/introduction\">https://inference-docs.cerebras.ai/introduction</a> 查看可以使用的模型</li>\n<li>目前只有 llama 系列</li>\n</ul>\n<h2 id=\"使用-2\"><a class=\"anchor\" href=\"#使用-2\">#</a> 使用</h2>\n<p>可以在首頁<a href=\"https://inference.cerebras.ai/\">https://inference.cerebras.ai/</a>直接使用</p>\n<p><img loading=\"lazy\" src=\"/images/AI/Accelerators/cerebras_home.webp\" alt=\"\" width=\"80%\" /></p>\n<h2 id=\"api-2\"><a class=\"anchor\" href=\"#api-2\">#</a> API</h2>\n<p><a href=\"https://cloud.cerebras.ai/platform/\">https://cloud.cerebras.ai/platform/</a><br />\n首頁右上方點擊 Get API Key，<strong>需要填寫 Google 表單申請</strong>，目前免費層級有提供一定的額度使用，<br />\nAPI 兼容 openai 格式，修改 base_url 為 <code>https://api.cerebras.ai/v1</code> 和 <code>model</code> 後可以直接使用。</p>\n<h1 id=\"sambanova-介紹\"><a class=\"anchor\" href=\"#sambanova-介紹\">#</a> Sambanova 介紹</h1>\n<p>SambaNova 開發的產品是 SN40L ，Reconfigurable Dataflow Unit (RDU)，專為 AI 推理與訓練設計的整合式加速晶片。</p>\n<ul>\n<li>SambaNova 與其他競品的比較</li>\n<li><a href=\"https://sambanova.ai/blog/sn40l-chip-best-inference-solution\">來源</a></li>\n</ul>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">指標</th>\n<th style=\"text-align:center\">SambaNova SN40L</th>\n<th style=\"text-align:center\">Cerebras WSE-3</th>\n<th style=\"text-align:center\">Groq LPU</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">晶片數（70B 模型）</td>\n<td style=\"text-align:center\">16 晶片</td>\n<td style=\"text-align:center\">336 晶片（4 晶圓）</td>\n<td style=\"text-align:center\">576 晶片</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">算力密度</td>\n<td style=\"text-align:center\">優於 Groq 40 倍 / Cerebras 10 倍</td>\n<td style=\"text-align:center\">高算力但受限於 SRAM 與多晶圓管線並行成本</td>\n<td style=\"text-align:center\">需大量晶片互連以補足 SRAM 容量限制</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">記憶體架構</td>\n<td style=\"text-align:center\">SRAM + HBM + DDR</td>\n<td style=\"text-align:center\">全 SRAM</td>\n<td style=\"text-align:center\">全 SRAM</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">量化需求</td>\n<td style=\"text-align:center\">無（16-bit 原生）</td>\n<td style=\"text-align:center\">無（官方宣稱使用 16-bit）</td>\n<td style=\"text-align:center\"><strong>推測</strong>需 int8 量化</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"可以使用的-models-3\"><a class=\"anchor\" href=\"#可以使用的-models-3\">#</a> 可以使用的 Models</h2>\n<ul>\n<li>可以在 <a href=\"https://docs.sambanova.ai/cloud/docs/get-started/supported-models\">https://docs.sambanova.ai/cloud/docs/get-started/supported-models</a> 查看可以使用的模型</li>\n<li>目前只有 DeepSeek、llama、Qwen 系列</li>\n<li>DeepSeek R1 671B 需要填表申請</li>\n</ul>\n<h2 id=\"使用-3\"><a class=\"anchor\" href=\"#使用-3\">#</a> 使用</h2>\n<p>可以在官方網站 <a href=\"https://cloud.sambanova.ai/\">playground</a>直接使用<br />\n<img loading=\"lazy\" src=\"/images/AI/Accelerators/sambanova_home.webp\" alt=\"\" width=\"80%\" /></p>\n<h2 id=\"api-3\"><a class=\"anchor\" href=\"#api-3\">#</a> API</h2>\n<p><a href=\"https://cloud.sambanova.ai/apis\">https://cloud.sambanova.ai/apis</a><br />\n在左邊 API Keys 的選單中點擊 Create API Key，目前提供新用戶 5 美元(3 個月到期)，<br />\nAPI 兼容 openai 格式，修改 base_url 為 <code>https://api.sambanova.ai</code> 和 model 後可以直接使用。</p>\n",
            "tags": [
                "Groq",
                "Cerebras",
                "SambaNova",
                "Deepseek",
                "llama"
            ]
        }
    ]
}