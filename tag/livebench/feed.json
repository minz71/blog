{
    "version": "https://jsonfeed.org/version/1",
    "title": "minz的筆記本 • All posts by \"livebench\" tag",
    "description": "這是一個分享我學習筆記的空間",
    "home_page_url": "https://blog.minz.li",
    "items": [
        {
            "id": "https://blog.minz.li/AI/BenchMark/",
            "url": "https://blog.minz.li/AI/BenchMark/",
            "title": "分享一些 AI 模型評測排名網站",
            "date_published": "2025-04-01T16:00:00.000Z",
            "content_html": "<h1 id=\"介紹\"><a class=\"anchor\" href=\"#介紹\">#</a> 介紹</h1>\n<p>隨著 AI 大語言模型的快速迭代，能力也增強了許多，可以在以下的網站查看各個模型的排名。</p>\n<h1 id=\"lmsysorg\"><a class=\"anchor\" href=\"#lmsysorg\">#</a> <a href=\"http://LMSYS.Org\">LMSYS.Org</a></h1>\n<p>LMSYS Org (Large Model Systems Organization)是一個開放的研究組織，由 UC Berkeley 的學生和教師與 UCSD 和 CMU 合作創立。<br />\n旨在通過共同開發開放模型、數據集、系統和評估工具，使大型模型對所有人都能夠接觸和使用。<br />\n希望建立一個開放和透明的平臺，提供語言模型基準數據和評估工具，幫助研究人員和開發者更好地理解和提升他們的模型性能。</p>\n<h2 id=\"chatbot-arena\"><a class=\"anchor\" href=\"#chatbot-arena\">#</a> Chatbot Arena</h2>\n<p><div class=\"links\"><div class=\"item\" title=\"LMSYS\" style=\"--block-color:#e9546b;\"><a href=\"https://lmarena.ai/leaderboard\" class=\"image\" data-background-image=\"https://lmarena.ai/images/favicon-light.ico\"></a>\n        <div class=\"info\">\n        <a href=\"https://lmarena.ai/leaderboard\" class=\"title\">LMSYS</a>\n        <p class=\"desc\">https://lmarena.ai/leaderboard</p>\n        </div></div></div></p>\n<p><img loading=\"lazy\" src=\"/images/AI/BenchMark/lmsysMode.webp\" alt=\"選擇模式\" title=\"選擇模式\" /></p>\n<ol>\n<li>⚔️ Arena (battle)<br />\n使用者可以輸入問題，會隨機提供不同的模型回答，使用者可以對回答較佳的模型投票，投票後會顯示剛剛回答的模型。</li>\n<li>⚔️ Arena (side-by-side)<br />\n可以讓使用者選擇2個<strong>指定</strong>的模型，使用者可以對回答較佳的模型投票。</li>\n<li>💬 Direct Chat<br />\n使用者可以直接和模型對話，可以選擇指定的模型。</li>\n</ol>\n<p><img loading=\"lazy\" src=\"/images/AI/BenchMark/lmsysArena1.webp\" alt=\"向模型提問的範例\" title=\"向模型提問的範例\" width=\"70%\" /></p>\n<h2 id=\"llm-排行榜\"><a class=\"anchor\" href=\"#llm-排行榜\">#</a> LLM 排行榜</h2>\n<p><a href=\"http://lmsys.org\">lmsys.org</a> 每隔一段時間都會發布排行榜，可以做為模型能力的參考，可以選擇不同的分類，查看該分類的排行榜。</p>\n<p><img loading=\"lazy\" src=\"/images/AI/BenchMark/lmsysLeaderBoard.webp\" alt=\"\" title=\"LLM 排行榜\" width=\"70%\" /></p>\n<h2 id=\"贊助商\"><a class=\"anchor\" href=\"#贊助商\">#</a> 贊助商</h2>\n<p><img loading=\"lazy\" src=\"/images/AI/BenchMark/lmsysDonations.webp\" alt=\"\" title=\"LMSYS Donation\" width=\"50%\" /><br />\n可以看到有許多大模型公司都贊助了 LMSYS，且會提供最新(或是測試中未公開)的語言模型給 LMSYS 測試。<br />\n模型未公開時會使用代號名稱，例如當時流傳的 m-also-a-good-gpt2-chatbot，也就是現在的 GPT-4o。</p>\n<h2 id=\"用戶能判斷模型的能力嗎\"><a class=\"anchor\" href=\"#用戶能判斷模型的能力嗎\">#</a> 用戶能判斷模型的能力嗎?</h2>\n<p>在 GPT-4o-mini 勝出其他大模型後，許多人對這個榜單提出質疑，因此提供用戶的<a href=\"https://huggingface.co/spaces/lmsys/gpt-4o-mini_battles\">對話與投票紀錄</a>，<br />\n<a href=\"https://lmsys.org/blog/2024-08-28-style-control/\">之後也提出 <code>style control</code> 功能</a>，目標是要有辦法區分模型分數是來自<code>內容</code>或是<code>風格</code>。</p>\n<h1 id=\"aider-leaderboards\"><a class=\"anchor\" href=\"#aider-leaderboards\">#</a> Aider Leaderboards</h1>\n<p><div class=\"links\"><div class=\"item\" title=\"Aider Leaderboards\" style=\"--block-color:#e9546b;\"><a href=\"https://aider.chat/docs/leaderboards/\" class=\"image\" data-background-image=\"https://aider.chat/assets/icons/favicon-32x32.png\"></a>\n        <div class=\"info\">\n        <a href=\"https://aider.chat/docs/leaderboards/\" class=\"title\">Aider Leaderboards</a>\n        <p class=\"desc\">https://aider.chat/docs/leaderboards/</p>\n        </div></div></div></p>\n<ul>\n<li>Aider 是一個開源的 AI 程式碼撰寫工具。</li>\n</ul>\n<p>Aider LLM Leaderboards 是由 Aider 開發的一組基準測試，用於評估大型語言模型（LLM）編輯程式碼的能力。<br />\n這些基準測試專注於模型編輯<strong>現有程式碼</strong>的能力，確保模型能夠一致地遵循系統提示詞（Prompt）完成編輯任務，適合評估在程式碼編輯的表現。</p>\n<p><img loading=\"lazy\" src=\"/images/AI/BenchMark/aider_leaderboards.webp\" alt=\"\" title=\"Aider Leaderboards\" width=\"80%\" /></p>\n<h2 id=\"如何進行基準測試\"><a class=\"anchor\" href=\"#如何進行基準測試\">#</a> 如何進行基準測試？</h2>\n<p>基準測試是 <a href=\"https://aider.chat/2024/12/21/polyglot.html#the-polyglot-benchmark\">Aider’s polyglot benchmark</a>，選取 Exercism 的 225 個困難的程式練習，測量模型<strong>完成任務</strong>的百分比以及使用<strong>正確編輯格式</strong>的百分比。</p>\n<ul>\n<li>diff 輸出程式碼的部分更改</li>\n<li>whole 輸出整個程式碼文件</li>\n</ul>\n<h1 id=\"livebench\"><a class=\"anchor\" href=\"#livebench\">#</a> LiveBench</h1>\n<p><div class=\"links\"><div class=\"item\" title=\"LiveBench\" style=\"--block-color:#e9546b;\"><a href=\"https://livebench.ai/\" class=\"image\" data-background-image=\"https://livebench.ai/favicon.ico\"></a>\n        <div class=\"info\">\n        <a href=\"https://livebench.ai/\" class=\"title\">LiveBench</a>\n        <p class=\"desc\">https://livebench.ai/</p>\n        </div></div></div></p>\n<p>LiveBench 是一個用於評估大型語言模型（LLM）的基準測試平台，目標是防止測試集污染並提供客觀評分。</p>\n<ul>\n<li>每月更新問題，確保模型沒有見過這些問題</li>\n<li>可以自行驗證分數，也可以接入模型 API 進行跑分</li>\n</ul>\n<p><img loading=\"lazy\" src=\"/images/AI/BenchMark/LiveBench.webp\" alt=\"\" title=\"LiveBench\" width=\"80%\" /></p>\n<h1 id=\"livecodebench\"><a class=\"anchor\" href=\"#livecodebench\">#</a> LiveCodeBench</h1>\n<p><div class=\"links\"><div class=\"item\" title=\"LiveCodeBench\" style=\"--block-color:#e9546b;\"><a href=\"https://livecodebench.github.io/leaderboard.html\" class=\"image\" data-background-image=\"https://livecodebench.github.io/images/favicon.svg\"></a>\n        <div class=\"info\">\n        <a href=\"https://livecodebench.github.io/leaderboard.html\" class=\"title\">LiveCodeBench</a>\n        <p class=\"desc\">https://livecodebench.github.io/leaderboard.html</p>\n        </div></div></div></p>\n<p>LiveCodeBench 是由加州大學柏克萊分校、MIT、康乃爾大學等團隊共同開發的程式碼大型語言模型（Code LLMs）評估基準</p>\n<ul>\n<li>持續更新的題目庫，<strong>避免資料污染</strong>的模型效能評測</li>\n<li>使用 <strong>LeetCode、AtCoder、Codeforces</strong> 平台的程式競賽題目</li>\n<li>可以調整題目時間，確保在模型的訓練截止日期之後</li>\n</ul>\n<p><img loading=\"lazy\" src=\"/images/AI/BenchMark/LiveCodeBench.webp\" alt=\"\" title=\"LiveCodeBench\" width=\"80%\" /></p>\n<h1 id=\"vectara-的模型幻覺排行榜\"><a class=\"anchor\" href=\"#vectara-的模型幻覺排行榜\">#</a> Vectara 的模型幻覺排行榜</h1>\n<p><div class=\"links\"><div class=\"item\" title=\"hallucination-leaderboard\" style=\"--block-color:#e9546b;\"><a href=\"https://github.com/vectara/hallucination-leaderboard\" class=\"image\" data-background-image=\"https://avatars.githubusercontent.com/u/108304503?s=200&v=4\"></a>\n        <div class=\"info\">\n        <a href=\"https://github.com/vectara/hallucination-leaderboard\" class=\"title\">hallucination-leaderboard</a>\n        <p class=\"desc\">https://github.com/vectara/hallucination-leaderboard</p>\n        </div></div></div></p>\n<p>透過 <a href=\"https://huggingface.co/vectara/hallucination_evaluation_model\">Hughes Hallucination Evaluation Model (HHEM)</a> 來測試模型幻覺。</p>\n<p>所謂「幻覺」（hallucinated）或「事實不一致」（factually inconsistent），是指<strong>一段文本（待判斷的假設）無法由源文本（給定的證據/前提）所支持</strong>。在檢索增強生成（RAG）的情境中，模型會從資料集中檢索到多段文本（通常稱為事實或上下文），若生成的摘要（假設）與這些源文本（給定的證據/前提）不符，即構成幻覺。</p>\n<p>RAG中一種幻覺類型是，LLM 生成的陳述在現實世界中是正確的，但是並未出現在提供的源文本。</p>\n<ul>\n<li>檢索到的事實（前提）：「法國的首都是柏林」(源文本內容)</li>\n<li>LLM 回答的摘要（假設）：「法國的首都是巴黎」(符合真實世界知識)</li>\n</ul>\n<div class=\"note default\">\n<p>這表示 LLM 未依賴 RAG 提供的資料，反而仰賴預訓練時學到的知識。</p>\n</div>\n<h1 id=\"fictionlive-評估模型在長上下文上的能力\"><a class=\"anchor\" href=\"#fictionlive-評估模型在長上下文上的能力\">#</a> fiction.live 評估模型在長上下文上的能力</h1>\n<p><div class=\"links\"><div class=\"item\" title=\"fiction.live\" style=\"--block-color:#e9546b;\"><a href=\"https://fiction.live/stories?terms=Fiction.liveBench&page=1\" class=\"image\" data-background-image=\"https://fiction.live/staticAssets/favicon.ico\"></a>\n        <div class=\"info\">\n        <a href=\"https://fiction.live/stories?terms=Fiction.liveBench&page=1\" class=\"title\">fiction.live</a>\n        <p class=\"desc\">https://fiction.live/stories?terms=Fiction.liveBench&page=1</p>\n        </div></div></div></p>\n<p>核心目標： 測試 AI 模型在處理長篇文本時，能否維持回答品質，並深度理解動態發展的故事情節。</p>\n<h2 id=\"測試數據來源\"><a class=\"anchor\" href=\"#測試數據來源\">#</a> 測試數據來源</h2>\n<ul>\n<li>基於 <strong>十幾部超長且複雜的故事（來自 Fiction.live 的真實用戶內容）</strong>，並結合人工驗證的問答題（quizzes）。</li>\n<li>實際的測試數據集（包含這些問題範例）是保密的。</li>\n<li>測試時，會對原始故事進行<strong>分段裁剪</strong>，生成不同長度的版本：\n<ul>\n<li><strong>0-token 測試</strong>：僅保留與問題直接相關的片段（最小上下文）。</li>\n<li><strong>逐步增加上下文長度</strong>：在相關內容周圍逐漸加入更多原始故事文本（即增加無關信息），測試模型在更長、更複雜的上下文中的表現。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"問題設計\"><a class=\"anchor\" href=\"#問題設計\">#</a> 問題設計</h2>\n<ul>\n<li>\n<p>具有難度梯度</p>\n<ul>\n<li>同一個問題會提供短上下文版本（如 1k tokens），多數模型能答對。</li>\n<li>但是長上下文版本（如 8k tokens），這對多數模型來說則非常困難。</li>\n</ul>\n</li>\n<li>\n<p>問題類型旨在測試<strong>深度理解能力</strong>，</p>\n<ul>\n<li>追蹤角色關係和動機的動態變化（例如，從恨轉變為愛，再到執念）。</li>\n<li>基於故事中的隱晦線索進行邏輯推理。</li>\n<li>區分讀者知曉但角色未知的秘密。</li>\n</ul>\n</li>\n<li>\n<p>刻意設計<strong>無法單靠搜索解決</strong>的問題：</p>\n<ul>\n<li>避免模型僅依賴關鍵字搜索來定位答案，強制模型必須真正閱讀和理解整個上下文。</li>\n<li>這更貼近小說寫作中對理解潛台詞、伏筆和情感細微變化的要求。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"模型評估方式\"><a class=\"anchor\" href=\"#模型評估方式\">#</a> 模型評估方式</h2>\n<ul>\n<li>橫向比較不同模型在不同上下文長度（如 1k, 8k tokens 等）下的表現。</li>\n<li>關鍵指標是準確率 (accuracy)，觀察其隨上下文增長而如何變化或衰減。</li>\n<li>傳統測試聚焦<strong>從長文中找答案</strong>，而 Fiction.LiveBench 測試<strong>綜合理解能力</strong>。\n<ul>\n<li>模型不僅要能找到某句對話，更要能理解角色關係如何隨時間演變。</li>\n</ul>\n</li>\n<li><strong>更貼近真實寫作需求</strong>，測試結果反映模型在<strong>創作輔助</strong>（如生成連貫的角色分析、維持情節一致性）中的實用性。</li>\n</ul>\n<h1 id=\"artificial-analysis-image-arena-leaderboard\"><a class=\"anchor\" href=\"#artificial-analysis-image-arena-leaderboard\">#</a> Artificial Analysis Image Arena Leaderboard</h1>\n<ul>\n<li>採取用戶投票的 AI 繪圖模型排行榜</li>\n</ul>\n<p><div class=\"links\"><div class=\"item\" title=\"Artificial Analysis Image Arena Leaderboard\" style=\"--block-color:#e9546b;\"><a href=\"https://artificialanalysis.ai/text-to-image/arena?tab=Leaderboard\" class=\"image\" data-background-image=\"https://artificialanalysis.ai/favicon.ico\"></a>\n        <div class=\"info\">\n        <a href=\"https://artificialanalysis.ai/text-to-image/arena?tab=Leaderboard\" class=\"title\">Artificial Analysis Image Arena Leaderboard</a>\n        <p class=\"desc\">https://artificialanalysis.ai/text-to-image/arena?tab=Leaderboard</p>\n        </div></div></div></p>\n",
            "tags": [
                "llm",
                "benchmark",
                "LMSYS",
                "Chatbot Arena",
                "livebench",
                "aider"
            ]
        }
    ]
}