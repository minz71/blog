{
    "version": "https://jsonfeed.org/version/1",
    "title": "minz的筆記本 • All posts by \"livebench\" tag",
    "description": "",
    "home_page_url": "https://blog.minz.li",
    "items": [
        {
            "id": "https://blog.minz.li/AI/AI_BenchMark/",
            "url": "https://blog.minz.li/AI/AI_BenchMark/",
            "title": "AI 大語言模型評測網站",
            "date_published": "2025-02-23T16:00:00.000Z",
            "content_html": "<h1 id=\"介紹\"><a class=\"anchor\" href=\"#介紹\">#</a> 介紹</h1>\n<p>隨著 AI 大語言模型的快速迭代，能力也增強了許多，可以在以下的網站查看目前模型的排名。</p>\n<h1 id=\"lmsysorg\"><a class=\"anchor\" href=\"#lmsysorg\">#</a> <a href=\"http://LMSYS.Org\">LMSYS.Org</a></h1>\n<p>LMSYS Org (Large Model Systems Organization) 是一個開放的研究組織，由 UC Berkeley 的學生和教師與 UCSD 和 CMU 合作創立。<br />\n旨在通過共同開發開放模型、數據集、系統和評估工具，使大型模型對所有人都能夠接觸和使用。<br />\n希望建立一個開放和透明的平臺，提供語言模型基準數據和評估工具，幫助研究人員和開發者更好地理解和提升他們的模型性能。</p>\n<div class=\"note info\">\n<p>本文主要介紹 Chatbot Arena，LMSYS Org 有其他的 projects。<br />\n詳情可參考: <a href=\"https://lmsys.org/projects/\">https://lmsys.org/projects/</a></p>\n</div>\n<h2 id=\"chatbot-arena\"><a class=\"anchor\" href=\"#chatbot-arena\">#</a> Chatbot Arena</h2>\n<div class=\"links\"><div class=\"item\" title=\"LMSYS\" style=\"--block-color:#e9546b;\"><a href=\"https://lmarena.ai/\" class=\"image\" data-background-image=\"https://lmarena.ai/favicon.jpeg\"></a>\n        <div class=\"info\">\n        <a href=\"https://lmarena.ai/\" class=\"title\">LMSYS</a>\n        <p class=\"desc\">arena.lmsys.org</p>\n        </div></div></div>\n<ol>\n<li>⚔️ Arena (battle)<br />\n 使用者可以輸入問題，會隨機提供不同的模型回答，使用者可以對回答較佳的模型投票，投票後會顯示剛剛回答的模型。</li>\n<li>⚔️ Arena (side-by-side)<br />\n 可以讓使用者選擇 2 個<strong>指定</strong>的模型，使用者可以對回答較佳的模型投票。</li>\n<li>💬 Direct Chat<br />\n 使用者可以直接和模型對話，可以選擇指定的模型。</li>\n</ol>\n<ul>\n<li>向模型提問的範例：</li>\n</ul>\n<figure class=\"highlight plain\"><figcaption data-lang=\"plain\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>今天有 3 個蘋果，我昨天吃了 1 個，請問還剩下幾個?</pre></td></tr></table></figure><p><img loading=\"lazy\" data-src=\"/assets/AI/AI_BenchMark/lmsysArena1.webp\" alt=\"詢問模型\" title=\"詢問模型\" width=\"80%\" /></p>\n<p><img loading=\"lazy\" data-src=\"/assets/AI/AI_BenchMark/lmsysArena2.webp\" alt=\"\" title=\"投票後\" width=\"80%\" /></p>\n<h2 id=\"llm-排行榜\"><a class=\"anchor\" href=\"#llm-排行榜\">#</a> LLM 排行榜</h2>\n<p><a href=\"http://lmsys.org\">lmsys.org</a> 每隔一段時間都會發布排行榜，可以做為模型能力的參考，可以選擇不同的分類，查看該分類的排行榜。</p>\n<p><img loading=\"lazy\" data-src=\"/assets/AI/AI_BenchMark/lmsysLeaderBoard.webp\" alt=\"\" title=\"LLM 排行榜\" width=\"80%\" /></p>\n<h2 id=\"贊助商\"><a class=\"anchor\" href=\"#贊助商\">#</a> 贊助商</h2>\n<p><img loading=\"lazy\" data-src=\"/assets/AI/AI_BenchMark/lmsysDonations.webp\" alt=\"\" title=\"LMSYS Donation\" width=\"60%\" /><br />\n可以看到有許多大公司都贊助了 LMSYS，<br />\n在新模型發布時，都會在第一時間提供給最新的語言模型，<br />\n例如當時流傳的 m-also-a-good-gpt2-chatbot，也就是現在的 GPT-4o。</p>\n<h2 id=\"用戶能判斷模型的能力嗎\"><a class=\"anchor\" href=\"#用戶能判斷模型的能力嗎\">#</a> 用戶能判斷模型的能力嗎？</h2>\n<p>在 GPT-4o-mini 勝出其他大模型後，許多人對這個榜單提出質疑，因此提供用戶的<a href=\"https://huggingface.co/spaces/lmsys/gpt-4o-mini_battles\">對話與投票紀錄</a>，<br />\n<a href=\"https://lmsys.org/blog/2024-08-28-style-control/\">之後也提出  <code>style control</code>  功能</a>，在模型 A 在內容上更優秀，而模型 B 在風格 (排版) 上更優秀，<br />\n目標是要有辦法區分模型分數是來自 <code>內容</code> 或是 <code>風格</code> 。</p>\n<h1 id=\"aider-leaderboards\"><a class=\"anchor\" href=\"#aider-leaderboards\">#</a> Aider Leaderboards</h1>\n<div class=\"links\"><div class=\"item\" title=\"Aider Leaderboards\" style=\"--block-color:#e9546b;\"><a href=\"https://aider.chat/docs/leaderboards/\" class=\"image\" data-background-image=\"https://aider.chat/assets/icons/favicon-32x32.png\"></a>\n        <div class=\"info\">\n        <a href=\"https://aider.chat/docs/leaderboards/\" class=\"title\">Aider Leaderboards</a>\n        <p class=\"desc\">https://aider.chat/docs/leaderboards/</p>\n        </div></div></div>\n<ul>\n<li>Aider 是一個開源的 AI 程式碼撰寫工具。</li>\n</ul>\n<p>Aider LLM Leaderboards 是由 Aider 開發的一組基準測試，用於評估大型語言模型（LLM）編輯程式碼的能力。<br />\n這些基準測試專注於模型編輯<strong>現有程式碼</strong>的能力，確保模型能夠一致地遵循系統提示詞（Prompt）完成編輯任務，適合評估在程式碼編輯的表現。</p>\n<p><img loading=\"lazy\" data-src=\"/assets/AI/AI_BenchMark/aider_leaderboards.webp\" alt=\"\" title=\"Aider Leaderboards\" width=\"80%\" /></p>\n<h2 id=\"如何進行基準測試\"><a class=\"anchor\" href=\"#如何進行基準測試\">#</a> 如何進行基準測試？</h2>\n<p>基準測試是 <a href=\"https://aider.chat/2024/12/21/polyglot.html#the-polyglot-benchmark\">Aider’s polyglot benchmark</a>，選取 Exercism 的 225 個困難的程式練習，測量模型<strong>完成任務</strong>的百分比以及使用<strong>正確編輯格式</strong>的百分比。</p>\n<ul>\n<li>diff 輸出程式碼的部分更改</li>\n<li>whole 輸出整個程式碼文件</li>\n</ul>\n<h1 id=\"livebench\"><a class=\"anchor\" href=\"#livebench\">#</a> LiveBench</h1>\n<div class=\"links\"><div class=\"item\" title=\"LiveBench\" style=\"--block-color:#e9546b;\"><a href=\"https://livebench.ai/\" class=\"image\" data-background-image=\"https://livebench.ai/favicon.ico\"></a>\n        <div class=\"info\">\n        <a href=\"https://livebench.ai/\" class=\"title\">LiveBench</a>\n        <p class=\"desc\">https://livebench.ai/</p>\n        </div></div></div>\n<p>LiveBench 是一個用於評估大型語言模型（LLM）的基準測試平台，目標是防止測試集污染並提供客觀評分。</p>\n<ul>\n<li>每月更新問題，確保模型沒有見過這些問題</li>\n<li>可以自行驗證分數，也可以接入模型 API 進行跑分</li>\n</ul>\n<p><img loading=\"lazy\" data-src=\"/assets/AI/AI_BenchMark/LiveBench.webp\" alt=\"\" title=\"LiveBench\" width=\"80%\" /></p>\n<h1 id=\"livecodebench\"><a class=\"anchor\" href=\"#livecodebench\">#</a> LiveCodeBench</h1>\n<div class=\"links\"><div class=\"item\" title=\"LiveCodeBench\" style=\"--block-color:#e9546b;\"><a href=\"https://livecodebench.github.io/leaderboard.html\" class=\"image\" data-background-image=\"https://livecodebench.github.io/images/favicon.svg\"></a>\n        <div class=\"info\">\n        <a href=\"https://livecodebench.github.io/leaderboard.html\" class=\"title\">LiveCodeBench</a>\n        <p class=\"desc\">https://livecodebench.github.io/leaderboard.html</p>\n        </div></div></div>\n<p>LiveCodeBench 是由加州大學柏克萊分校、MIT、康乃爾大學等團隊共同開發的程式碼大型語言模型（Code LLMs）評估基準</p>\n<ul>\n<li>持續更新的題目庫，<strong>避免資料污染</strong>的模型效能評測</li>\n<li>使用 <strong>LeetCode、AtCoder、Codeforces</strong> 平台的程式競賽題目</li>\n<li>可以調整題目時間，確保在模型的訓練截止日期之後</li>\n</ul>\n<p><img loading=\"lazy\" data-src=\"/assets/AI/AI_BenchMark/LiveCodeBench.webp\" alt=\"\" title=\"LiveCodeBench\" width=\"80%\" /></p>\n",
            "tags": [
                "llm",
                "benchmark",
                "LMSYS",
                "Chatbot Arena",
                "livebench",
                "aider"
            ]
        }
    ]
}